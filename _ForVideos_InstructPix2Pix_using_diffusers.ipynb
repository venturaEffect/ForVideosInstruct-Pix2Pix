{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venturaEffect/ForVideosInstruct-Pix2Pix/blob/main/_ForVideos_InstructPix2Pix_using_diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First you need to download ffmpeg: https://ffmpeg.org/download.htmlWhen to convert video into frames and back.\n",
        "\n",
        "When you have the video you want to edit. Save it on a folder. On that folder create a '.sh' file. For example 'videotopng.sh'. You will need to know how many frames per second the video has. For that go to properties on the video and you'll get the information. Now, on the 'videotopng.sh' file write:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hXJYJYfH_odP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ffmpeg -i yourmovie.mp4 -r 30/1 %04d.png**"
      ],
      "metadata": {
        "id": "cCXz8eGcVZrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the '30/1' means that is 30 frames per second. the '%04d.png' tells to count with four digits. Example: not '1.png' instead '0001.png'. This is useful if you want to use EbSynth on the process also."
      ],
      "metadata": {
        "id": "oXs46q_sA3Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have created the frames on the same folder as the video and the 'videotopng.sh' file, upload the folder to your Google Drive. Run the next cell. It will connect this notebook with your Google Drive."
      ],
      "metadata": {
        "id": "eWp_rq7kBTMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_3efvkfLWiRe",
        "outputId": "b240fdbd-0597-4605-d9fa-efcd1b3b8854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# InstructPix2Pix: Learning to Follow Image Editing Instructions\n",
        "\n",
        "A demo notebook for [InstructPix2Pix](https://www.timothybrooks.com/instruct-pix2pix/) using [diffusers](https://github.com/huggingface/diffusers). InstructPix2Pix is fine-tuned stable diffusion model which allows you to edit images using language instructions.\n",
        "\n",
        "<img src='https://instruct-pix2pix.timothybrooks.com/teaser.jpg'/>"
      ],
      "metadata": {
        "id": "1aRurg5CBe82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the next cell\n"
      ],
      "metadata": {
        "id": "-Koq49MeB1RG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PljIJ5ml232h"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq git+https://github.com/huggingface/diffusers.git gradio transformers accelerate safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `StableDiffusionInstructPix2PixPipeline` pipeline"
      ],
      "metadata": {
        "id": "a6zlpKXXC4ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
        "\n",
        "model_id = \"timbrooks/instruct-pix2pix\"\n",
        "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, revision=\"fp16\", safety_checker=None)\n",
        "pipe.to(\"cuda\")\n",
        "pipe.enable_attention_slicing()"
      ],
      "metadata": {
        "id": "oV5IrvEu3rkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an array with all the files with the CORRECT PATH. ALSO IMPORTANT: Change the range from 1 to the number of frames it has your folder. Example: 1 to 2000 frames + 1\n"
      ],
      "metadata": {
        "id": "qneNsdwVW66r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "for i in range(1, TOTAL FRAMES + 1):\n",
        "       files.append(str(\"./drive/MyDrive/YOUR-PATH-TO-THE-FOLDER/\") + str('{0:04}'.format(i)) + str(\".png\"))\n",
        "      \n",
        "print(files)"
      ],
      "metadata": {
        "id": "4ZU3Z-RhXALY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now loop through all the frames and make the trick of instruct-pix2pix changing each frame. write on the prompt how you want the outlook. Remember to create a folder where these new frames will go on your Drive and write the correct path on 'generated_image.save(...)'"
      ],
      "metadata": {
        "id": "wrOqgCHmCjfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the list and open each image\n",
        "for file in files:\n",
        "    with PIL.Image.open(file) as image:\n",
        "        # Perform operations on the image\n",
        "        image = PIL.ImageOps.exif_transpose(image)\n",
        "        image = image.convert(\"RGB\")\n",
        "        prompt = \"WRITE HERE THE PROMPT\"\n",
        "        generated_image = pipe(prompt, image=image, num_inference_steps=20, image_guidance_scale=1).images[0]\n",
        "        # Save the generated image to Google Drive\n",
        "        generated_file = file[-8:]\n",
        "        generated_image.save(f'./drive/MyDrive/FOLDER-WHERE-THE-RESULTS-GOES/{generated_file}')"
      ],
      "metadata": {
        "id": "VZ4a7tsQYpEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once all the frames are done, download the Folder FROM Drive on your computer. On the folder create a file called 'pngtovideo.sh' and write iside the folder:\n",
        "\n"
      ],
      "metadata": {
        "id": "a7-ERQGhU9to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ffmpeg -framerate 30 -pattern_type glob -i '*.png' -c:v libx264 -pix_fmt yuv420p VIDEO_OUT.mp4**"
      ],
      "metadata": {
        "id": "RW64Lsy7Vf5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here 30 are the frames per second. Change it depending on the frames from the original video. And VIDEO_OUT.mp4 is how the video will be called.\n",
        "\n",
        "Now, to make the frames into video, open the terminal on the folder where the frames and 'pngtovideo.sh' is and write on the terminal 'sh pngtovideo.sh'.\n",
        "\n",
        "With this you have the output video."
      ],
      "metadata": {
        "id": "xkFUmnmdViYs"
      }
    }
  ]
}